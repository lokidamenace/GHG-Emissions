# -*- coding: utf-8 -*-
"""La_Peter_GHG_Emissions_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yizRgoyq5cZyo1APbJTPy79oNIC9s_B7

### Greenhouse Gas Emissions Project

Author: Peter La

Title: Predicting Trends in Greenhouse Gas Emissions

Date: May 4, 2025

Author: Peter la

Purpose: </br>
Unsupervised learning: Explore and analyze greenhouse gas emissions data to reveal hidden patterns or insights without predefined labels. </br>
Data visualization: Visualize emission trends across different countries.

Input: ghg_post_outlier.csv

Outputs:
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import os
import warnings

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
import seaborn as sns;

warnings.filterwarnings("ignore")
# %matplotlib inline
dpi = 300
sns.set()

from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import SpectralClustering

"""### Import Dataset"""

df = pd.read_csv(r'/content/drive/MyDrive/ghg_data/ghg_post_outlier.csv')
df.sample(20)

"""### Data Preparation"""

df.info()

df = df.drop('account_name', axis=1)

df = df.drop('account_id', axis=1)

df = df.drop('Third_party_verification', axis=1)

df.info()

categorical_columns = ['incorporated_country', 'Primary activity', 'Primary sector', 'Scope_3_emissions_type']

label_encoder = LabelEncoder()
for column in categorical_columns:
    df[column] = label_encoder.fit_transform(df[column])

df.info()

df.describe()

df.to_csv('/content/drive/MyDrive/ghg_data/prepped_ghg_post_outlier.csv', index=False)

df = pd.read_csv(r'/content/drive/MyDrive/ghg_data/prepped_ghg_post_outlier.csv')

path = '/content/drive/MyDrive/ghg_data/'

"""### Exploratory Data Analysis"""

correlation_matrix = df.corr()
correlation_matrix

"""Let's look at average emissions trend per year."""

# Calculate average emissions per year
avg_emissions_per_year = df.groupby('Year')['Scope_3_emissions_amount'].mean().reset_index()

# Plot the line chart
plt.figure(figsize=(10, 6))
sns.lineplot(data=avg_emissions_per_year, x='Year', y='Scope_3_emissions_amount', marker='o')
plt.title('Average Scope 3 Emissions per Year')
plt.xlabel('Year')
plt.ylabel('Average Scope 3 Emissions')
plt.grid(False)
plt.tight_layout()
plot_filename = 'linechart.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

"""There is a gradual increase in average emissions over time from 2021 to 2023.  This upward pattern could reflect economic recovery or growth in high-emission sectors.

Let's look at the distribution of Scope 3 Emissions with a histogram to understand the distribution and potential skewness.
"""

# Histogram for Scope 3 Emissions distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['Scope_3_emissions_amount'], bins=50, kde=True, log_scale=True)
plt.title('Distribution of Scope 3 Emissions Amount (Log Scale)')
plt.xlabel('Scope 3 Emissions Amount (log scale)')
plt.ylabel('Frequency')
plt.grid(False)
plt.tight_layout()
plot_filename = 'emissiondistribution.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

"""Linear scale is useful when you're interested in absolute values and the spread is moderate. However, for datasets like this with large variation, the log scale is more informative for highlighting underlying relationships."""

# Setup subplots
fig, axes = plt.subplots(figsize=(20, 6))

# Boxplot of emissions by Primary Sector
sns.boxplot(x='Primary sector', y='Scope_3_emissions_amount', data=df, palette="Spectral")
axes.set_yscale('log')
axes.set_title('Scope 3 Emissions by Primary Sector (Log Scale)')
axes.set_xlabel('Primary Sector')
axes.set_ylabel('Scope 3 Emissions (log scale)')

"""Some sectors display significantly higher median emissions and a broader range, indicating that emissions are strongly sector-dependent."""

plot_filename = 'boxplots.png'
fig.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)

"""Let's break down the emissions by sector over time to pinpoint specific contributors."""

# Group by year and primary sector to calculate average emissions per sector over time
sector_emissions_over_time = df.groupby(['Year', 'Primary sector'])['Scope_3_emissions_amount'].mean().reset_index()
sector_avg_emissions = sector_emissions_over_time.groupby('Primary sector')['Scope_3_emissions_amount'].mean()
highest_emission_sector = sector_avg_emissions.idxmax()

# Plot the line chart with multiple lines, one for each sector
plt.figure(figsize=(14, 8))
plot = sns.lineplot(
    data=sector_emissions_over_time,
    x='Year',
    y='Scope_3_emissions_amount',
    hue='Primary sector',
    palette='tab10',
    linewidth=2,
    legend=False
)

# Annotate the sector with the highest average emissions
top_sector_data = sector_emissions_over_time[sector_emissions_over_time['Primary sector'] == highest_emission_sector]
top_latest_point = top_sector_data[top_sector_data['Year'] == top_sector_data['Year'].max()]
for _, row in top_latest_point.iterrows():
    plt.text(row['Year'] + 0.1, row['Scope_3_emissions_amount'], f'Highest: Sector {int(row["Primary sector"])}',
             color='black', fontsize=11, weight='bold')

plt.title('Average Scope 3 Emissions by Primary Sector Over Time')
plt.xlabel('Year')
plt.ylabel('Average Scope 3 Emissions')
plt.grid(False)
plt.tight_layout()
plot_filename = 'emissionsbysector.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

"""The plot identifies a sector with the highest average emission rate over time. Looking back at the original dataset before preprocessing, sector 34 is oil & gas processing (refineries).

Let's plot a heatmap for all variables.
"""

# Plot heatmap of correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='crest', linewidths=0.5)
plt.title('Correlation Between Greenhouse Gas Data Variables')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plot_filename = 'heatmap_allvariables.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

"""Let's create a heatmap to specifically show correlations among financial metrics and emissions."""

# Numerical columns for correlation analysis
numerical_columns = [
    'Market_Cap_USD', 'Revenue_USD', 'ebitda_USD', 'grossProfit_USD',
    'netIncome_USD', 'totalAssets_USD', 'totalLiabilities_USD',
    'totalDebt_USD', 'totalEquity_USD', 'Scope_3_emissions_amount',
    'country_ghg_avg', 'country_population_avg', 'country_gdp_avg'
]

# Compute correlation matrix
correlation_matrix_2 = df[numerical_columns].corr()

# Plot heatmap of correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix_2, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix of Financial Metrics and Emissions')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plot_filename = 'financialmatrix.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

"""There are strong positive correlations among financial metrics, are also strongly interrelated. Emissions (Scope_3_emissions_amount) have notably weak correlations with most financial metrics, suggesting emissions might be influenced by other factors. The country_ghg_avg variable strongly correlates with both country_population_avg and country_gdp_avg. That could indicates some relationship that could be explain with further analysis.

Based on this heatmap, we will select the relevant features for clustering analysis to identify groups of countries or companies based on their emissions profile.
"""

from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# Select relevant features for clustering
features = [
    'Scope_3_emissions_amount', 'Market_Cap_USD', 'Revenue_USD', 'netIncome_USD',
    'totalAssets_USD', 'totalLiabilities_USD', 'totalDebt_USD',
    'country_population_avg', 'country_gdp_avg', 'country_ghg_avg'
]

# Drop rows with missing values in selected features
ghg_cluster_data = df[features].dropna()

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit(ghg_cluster_data)
scaled_data = scaled_data.transform(ghg_cluster_data)

scaled_data = pd.DataFrame(scaled_data,columns=ghg_cluster_data.columns)
scaled_data.head(10)

# determine the optimal number of clusters
sum_sq= []

# fit the model for a range of 2 to 30 clusters and add to sum_sq[]
for n in range (2,30):
    print('Calculating for ',n,' clusters')

    model = KMeans(n_clusters=n, random_state=42)
    model.fit(scaled_data)
    sum_sq.append(-model.score(scaled_data))

# make our scree plot to visually see the optimal number of clusters
plt.plot(range(2, 30), sum_sq, 'bx-')

"""As we can see from the above graph that there is turning like an elbow at k=9. So, I can say that the estimated number of clusters for the given datasets is 9. Then, I'm rerunning with the optimal k value and gather the predictions."""

model = KMeans(n_clusters=9, random_state=42)
model.fit(scaled_data)

preds = model.predict(scaled_data)
preds

"""I'm going to look at the silhouette score to evaluate this first model."""

score = metrics.silhouette_score(scaled_data, preds)
score

"""This high silhouette score generally indicates that the clusters are well-separated. Let's apply PCA to reduce dimensionality for visualization."""

# Apply PCA to reduce dimensionality for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Apply K-means=9
kmeans = KMeans(n_clusters=9, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Prepare data for visualization
pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])
pca_df['Cluster'] = clusters

# Visualize PCA results with cluster labels
plt.figure(figsize=(10, 7))
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Cluster', palette='Set2', alpha=0.7)
plt.title('K-Means Clustering Results (2D PCA Projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.grid(False)
plt.tight_layout()
plot_filename = 'kmeansclustering.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

"""Although the elbow method indicates the number of optimal clusters is 9 in the dataset, I'm going to use the silhouette score to verify the the number of clusters."""

for n in range (2,30):
    model2 = KMeans(n_clusters=n, random_state=42)
    model2.fit(scaled_data)

    preds = model2.predict(scaled_data)
    score = metrics.silhouette_score(scaled_data, preds)
    print('Silhouette score for ',n,' clusters: ',score)

"""Based on the result, it looks like 2 clusters and 11 clusters performed the best. I'm going to rerun the model with k=2 and k=11 values and gather the predictions. And just to confirm, I will also visualize them in a PCA plot."""

model3 = KMeans(n_clusters=2, random_state=42)
model3.fit(scaled_data)

# gather the predictions
preds= model3.predict(scaled_data)

score = metrics.silhouette_score(scaled_data, preds)
score

# Apply PCA to reduce dimensionality for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Apply K-means=2
kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Prepare data for visualization
pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])
pca_df['Cluster'] = clusters

# Visualize PCA results with cluster labels
plt.figure(figsize=(10, 7))
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Cluster', palette='Set2', alpha=0.7)
plt.title('K-Means Clustering Results (2D PCA Projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.grid(False)
plt.tight_layout()
plot_filename = 'kmeansclustering2.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

model4 = KMeans(n_clusters=11, random_state=42)
model4.fit(scaled_data)

# gather the predictions
preds= model4.predict(scaled_data)

score = metrics.silhouette_score(scaled_data, preds)
score

# Apply PCA to reduce dimensionality for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Apply K-means=11
kmeans = KMeans(n_clusters=11, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Prepare data for visualization
pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])
pca_df['Cluster'] = clusters

# Visualize PCA results with cluster labels
plt.figure(figsize=(10, 7))
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Cluster', palette='Set2', alpha=0.7)
plt.title('K-Means Clustering Results (2D PCA Projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.grid(False)
plt.tight_layout()
plot_filename = 'kmeansclustering3.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()

# Labeling the data points with Sector ID
pca_df['Sector_ID'] = df.loc[ghg_cluster_data.index, 'Primary sector']

plt.figure(figsize=(10, 7))
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Sector_ID', palette='Set2', alpha=0.7)
plt.title('K-Means Clustering Results (2D PCA Projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Sector ID')
plt.grid(False)
plt.tight_layout()
plot_filename = 'clusteringwithsectorid.png'
plt.savefig(path + plot_filename, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
plt.show()





